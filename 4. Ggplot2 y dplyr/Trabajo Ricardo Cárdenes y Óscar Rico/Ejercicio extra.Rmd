---
title: "Actividad extra"
author: "Óscar Rico Rodríguez"
date: "`r Sys.Date()`"
output: html_document
---

```{=html}
<style>
body {
text-align: justify;
font-size: 12pt;
line-height: 200%}
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
knitr::opts_knit$set(echo = FALSE)
knitr::opts_knit$set(root.dir = "./datasets")
options(knitr.kable.NA = '')
```

```{r}
library(ggplot2)
library(dplyr)
library(lubridate)
library(gsubfn) #Regex en R
library(knitr) #Presentación de las tablas
library(zoo) #Tratamiento de NAs
library(scales) #Escala de colores
```

## Medir tiempo de ejecución

Para llevar a cabo esta tarea, he optado por utilizar el mismo conjunto de datos que empleamos en el primer trabajo. Esta elección se basa en la versatilidad del conjunto, el cual permite llevar a cabo diversas experimentaciones, y en que ya cuento con un tratamiento previo de los datos.

### Recoger los datos de todas las aulas

```{r echo=FALSE}
lee_co2 <- function (filename) {
 df <- read.table(filename, header = T, sep = "\t", dec = ",", fileEncoding = "LATIN1")
 df$Aula <- paste("Aula",strapplyc(filename, "\\d\\W\\d", simplify = TRUE))
 return(df)
}

files <- list.files(pattern="\\.txt$")

colnames = c(Temperatura = "Temperatura..ºC.", Humedad = "Humedad.Relativa....", CO2 = "CO2.ppm")
jaulas <- purrr::map(files,lee_co2) %>% 
  lapply(function(df) {
    df %>%
      rename(all_of(colnames)) %>%
      mutate(Humedad = sub(',', '\\.', Humedad)) %>%
      mutate(Humedad = as.numeric(Humedad))
    }) %>%
  bind_rows()
```

En un análisis exploratorio de datos, el primer paso es recopilar los datos necesarios. Una vez obtenidos los datos, es importante llevar a cabo una exploración inicial para determinar sus características más relevantes, como el tipo de variables, la cantidad de datos disponibles, las unidades de medida utilizadas, entre otros.

### Tratar NAs y valores atipicos

```{r echo=FALSE}
# Buscamos la existencia de NAs
kable(summary(jaulas))
```

Tras una primera inspección de los datos, podemos observar que se trata de un conjunto de datos considerable, con un total de 1.081.728 filas. Sin embargo, se ha identificado que existen algunas inconsistencias en los datos.

En particular, se ha detectado que los valores mínimos de las variables de *Humedad* y *Temperatura* no parecen tener sentido, lo que sugiere que podrían ser valores atípicos o erróneos. Además, se ha constatado que faltan tres valores en la variable *CO2*.

Es importante tener en cuenta estas inconsistencias y tomar medidas para abordarlas adecuadamente antes de proceder con el análisis de los datos. Esto podría incluir la eliminación de los valores atípicos, la imputación de los valores faltantes, o cualquier otra técnica de limpieza de datos necesaria para garantizar la calidad y fiabilidad de los resultados del análisis.

```{r echo=FALSE}
# Entre los datos a simple vista pudimos encontrar 3 valores faltas de CO_2 y a lo largo del proyecto, encontramos que faltaban 2 fechas también
kable(jaulas %>%
  filter(is.na(CO2) | Fecha == ""))
```

Hemos identificado que en el conjunto de datos hay tres valores faltantes para la variable *CO2*, así como dos para la variable *Fecha*. Tras evaluar cada caso individualmente, hemos tomado la decisión de descartar las filas sin fecha debido a la falta de información que presentan.

Sin embargo, hemos decidido recuperar los datos correspondientes al 08/03/22 y el 11/09/22. Para ello, hemos calculado la media entre el valor previo y posterior, ya que los intervalos entre los valores no superan los seis minutos. Consideramos que esta estrategia es adecuada ya que nos proporciona una aproximación bastante precisa de los valores faltantes.

Además, hemos decidido descartar una de la fila correspondiente al 12/04/22 debido a que los valores de *Temperatura* y *Humedad* parecen sugerir un posible fallo en el sensor. Al eliminar esta fila, evitamos posibles distorsiones en nuestro análisis y nos aseguramos de trabajar con datos fiables y representativos.

```{r echo=FALSE}
# Asignamos valor a los NA con Temperatura y Humedad razonables aplicando la media entre su valor anterior y posterior
jaulas <- jaulas %>%
  mutate(CO2 = na.approx(CO2))
```

```{r echo=FALSE}
# Eliminamos valor que no tiene sentido (sensor posiblemente roto)
jaulas <- jaulas %>%
  filter(!(Aula == "Aula 2-5" & Fecha == "12/04/22" & Hora == "09:35:53"))
```

```{r echo=FALSE}
# Eliminamos valores sin Fecha
jaulas <- jaulas %>%
  filter(Fecha != "")
```

```{r echo=FALSE}
kable(summary(jaulas))
```

Después de aplicar las técnicas de imputación y eliminación de valores faltantes correspondientes, se ha logrado obtener un conjunto de datos limpio y completo. Como resultado, todas las variables numéricas presentan valores razonables y no se observa la presencia de valores faltantes en el conjunto de datos.

### Medir el tiempo de ejecución

Para comparar el tiempo de ejecución entre datos resumidos y no resumidos, he decidido utilizar la función **Sys.time()** en lugar de **system.time()**. Esto se debe a que **system.time()** no refleja el tiempo de graficado de una gráfica, el cual es el paso que más tarda y resulta fundamental para medir el tiempo de ejecución total.

Para llevar a cabo esta comparación, he decidido replicar una gráfica utilizando las funciones **geom_line()** y **geom_point()** de la librería *ggplot2*. Para mejorar la visualización y hacer que este experimento sea lo más realista posible, utilizaré algunas funciones adicionales para decorar el gráfico. Esta gráfica mostrará las medias de los meses del año y será generada tanto para los datos resumidos como para los datos no resumidos.

#### Datos no resumidos

```{r echo=FALSE}
# Cargamos los nombres de los meses del año
o_mes_a = c('ene.', 'feb.', 'mar.', 'abr.','may.', 'jun.', 'jul.', 'ago.', 'sep.', 'oct.','nov.', 'dic.')
```

```{r fig.align='center'}
#Datos no resumidos
start = Sys.time()
jaulas %>%
  mutate(Mes = month(dmy(Fecha))) %>%
  group_by(Mes) %>%
  mutate(MeanCO2 = mean(CO2))  %>%
  ggplot(aes(factor(Mes, labels = o_mes_a),MeanCO2)) + 
  geom_line(aes(Mes,MeanCO2), size = 0.5) +
  geom_point(aes(color = factor(Mes)),size = 3) +
  scale_y_continuous(expand=c(0.03,0,0.05,0), breaks=seq(410,460,7.5)) +
  scale_x_discrete(expand=c(0.03,0,0.03,0)) + 
  labs(
    title = expression("Nivel medio de "*CO[2]*" a lo largo del año") ,
    subtitle = "En todas las aulas",
    x = "",
    y = expression(CO[2]*" (ppm)")) + 
  theme(
    plot.title=element_text(size=14, face='bold', hjust = 0.5),
    plot.subtitle = element_text(size=12, hjust = 0.5),
    legend.position = "None"
  )
end = Sys.time()
end - start
```

Como se puede observar, el tiempo de ejecución de la primera gráfica, a pesar de representar pocos datos, resulta inmenso, casi de un minuto de ejecución. Después de analizar el problema, llegué a la conclusión de que esto podría deberse a que cuando no resumimos los datos, lo que resulta en una pérdida de tiempo ya que no representa información adicional, solo grafica datos redundantes. Por ejemplo, en el caso de este conjunto de datos que cuenta con un total de 1.000.000 de datos, se estarían graficando 100.000.000 de puntos, cuando serían necesarios solo 12.

#### Datos resumidos

```{r fig.align='center'}
start = Sys.time()
  jaulas %>%
    mutate(Mes = month(dmy(Fecha))) %>%
    group_by(Mes) %>%
    summarise(MeanCO2 = mean(CO2)) %>%
    ggplot(aes(factor(Mes, labels = o_mes_a),MeanCO2)) + 
    geom_line(aes(Mes,MeanCO2), size = 0.5) +
    geom_point(aes(color = factor(Mes)),size = 3) +
    scale_y_continuous(expand=c(0.03,0,0.05,0), breaks=seq(410,460,7.5)) +
    scale_x_discrete(expand=c(0.03,0,0.03,0)) + 
    labs(title = expression("Nivel medio de "*CO[2]*" a lo largo del año") ,
         subtitle = "En todas las aulas",
         x = "",
         y = expression(CO[2]*" (ppm)")) + 
    theme(
    plot.title=element_text(size=14, face='bold', hjust = 0.5),
    plot.subtitle = element_text(size=12, hjust = 0.5),
    legend.position = "None"
    )
end = Sys.time()
end - start
```

En cambio, en la segunda gráfica el tiempo de ejecución es mínimo, no llega a un segundo de ejecución. Esto se debe a que, en este caso, se resumen los datos utilizando las funciones **group_by()** y **summarise()**, obteniendo un único valor por mes y solo es necesario graficar 12 puntos en total, lo que agiliza considerablemente el proceso.

### Conclusión

Luego de realizar los experimentos, podemos concluir que el uso de datos resumidos es altamente recomendable. Esto se debe a que, como pudimos comprobar, mejora significativamente el tiempo de ejecución. Además, en el caso la libería *dplyr*, resulta mucho más intuitivo y permite ahorrar memoria. Por otro lado, a la hora de revisar y corregir errores, trabajar con datos resumidos resulta mucho menos pesado y facilita el proceso.
